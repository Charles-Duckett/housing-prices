{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main csv file\n",
    "df = pd.read_csv('housing.csv')\n",
    "# auxiliary csv files\n",
    "cal_cities = pd.read_csv('cal_cities_lat_long.csv')\n",
    "cal_pops_cities = pd.read_csv('cal_populations_city.csv')\n",
    "cal_pops_counties = pd.read_csv('cal_populations_county.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ocean_proximity']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# 80% train, 20% test\n",
    "# random_state=42 for reproducibility\n",
    "# stratify=df['ocean_proximity'] to ensure that the train and test sets have the same proportion of each category as the full dataset\n",
    "# we also want to make sure that we dont include the median_house_value in the train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('median_house_value', axis=1)\n",
    "y = df['median_house_value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ocean_proximity'] == '<1H OCEAN', 'ocean_proximity'] = 'WITHIN HOUR TO OCEAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ocean_proximity']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['total_bedrooms']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the histogram, we can see that the data is skewed to the right\n",
    "df[['total_bedrooms']].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['total_bedrooms']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fill the missing values with the median\n",
    "df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # no more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have all our data, let's investigate even further\n",
    "# we'll start by looking at the correlation between the variables\n",
    "number_cols = df.select_dtypes(include=['int64', 'float64'])\n",
    "number_cols.drop(['longitude', 'latitude'], axis=1, inplace=True)\n",
    "number_cols.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a correlation matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(number_cols.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the scatter plots of the variables that are highly correlated\n",
    "sns.pairplot(number_cols[['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some feature engineering\n",
    "# we'll start by creating a new variable that will be the ratio of total_rooms to households\n",
    "df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "\n",
    "# let's create another variable that will be the ratio of total_bedrooms to total_rooms\n",
    "df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "\n",
    "# let's create another variable that will be the ratio of population to households\n",
    "df['population_per_household'] = df['population'] / df['households']\n",
    "\n",
    "# let's look at the correlation matrix again\n",
    "plt.figure(figsize=(12, 8))\n",
    "number_cols = df.select_dtypes(include=['int64', 'float64'])\n",
    "number_cols.drop(['longitude', 'latitude'], axis=1, inplace=True)\n",
    "sns.heatmap(number_cols.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is total_rooms still necessary?\n",
    "df.drop(['total_rooms'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is total_bedrooms still necessary?\n",
    "df.drop(['total_bedrooms'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is population still necessary?\n",
    "# df.drop(['population'], axis=1, inplace=True)\n",
    "# I think it's still necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is households still necessary?\n",
    "df.drop(['households'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the correlation matrix again\n",
    "plt.figure(figsize=(12, 8))\n",
    "number_cols = df.select_dtypes(include=['int64', 'float64'])\n",
    "number_cols.drop(['longitude', 'latitude'], axis=1, inplace=True)\n",
    "sns.heatmap(number_cols.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make geographical plots\n",
    "# let's start by looking at the geographical distribution of the median house value\n",
    "\n",
    "# Load the California shapefile\n",
    "file_path = os.path.join('CA_Counties', 'CA_Counties_TIGER2016.shp')\n",
    "cali_shp = gpd.read_file(file_path)\n",
    "\n",
    "# Create the GeoDataFrame for your points\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']), crs=cali_shp.crs)\n",
    "\n",
    "# Set the CRS of the points GeoDataFrame to match the CRS of the California GeoDataFrame\n",
    "gdf.set_crs(cali_shp.crs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California shapefile\n",
    "# cali = gpd.read_file('CA_Counties\\CA_Counties_TIGER2016.shp')\n",
    "\n",
    "file_path = os.path.join('CA_Counties', 'CA_Counties_TIGER2016.shp')\n",
    "cali_shp = gpd.read_file(file_path)\n",
    "cali_shp = cali_shp.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Create the GeoDataFrame for your points\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']), crs=cali_shp.crs)\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = cali_shp.plot(color='grey',figsize=(10, 10), alpha=0.4, edgecolor='blue')\n",
    "# gdf.plot(ax=ax, color='red')\n",
    "gdf.plot(ax=ax, kind='scatter', x='longitude', y='latitude', alpha=0.4, s=gdf['population']/500, label='population', figsize=(10,10), \n",
    "                        c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cali_shp.plot(color='grey',figsize=(10, 10), alpha=0.4, edgecolor='blue')\n",
    "gdf.plot(ax=ax, kind='scatter', x='longitude', y='latitude', alpha=0.4, s=gdf['median_income']/1, label='population', figsize=(10,10), \n",
    "                        c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the geographical distribution of the median income\n",
    "ax = cali_shp.plot(color='grey',figsize=(10, 10), alpha=0.4, edgecolor='blue')\n",
    "gdf.plot(ax=ax, kind='scatter', x='longitude', y='latitude', alpha=0.4, s=gdf['median_income']/1, label='population', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_pops_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_pops_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "CityTuple = namedtuple('City', ['Name', 'Latitude', 'Longitude','pop_april_1980', 'pop_april_1990', 'pop_april_2000', 'pop_april_2010'])\n",
    "\n",
    "city_map = dict()\n",
    "for index, row in cal_cities.iterrows():\n",
    "    city_map[row['Name']] = CityTuple(row['Name'], row['Latitude'], row['Longitude'], 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in cal_pops_cities.iterrows():\n",
    "    if row['City'] in city_map:\n",
    "        tuple_ = city_map[row['City']]\n",
    "        tuple_ = tuple_._replace(pop_april_1980=row['pop_april_1980'], pop_april_1990=row['pop_april_1990'], pop_april_2000=row['pop_april_2000'], pop_april_2010=row['pop_april_2010'])\n",
    "        city_map[row['City']] = tuple_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to create a geodataframe for the cities\n",
    "from shapely.geometry import Point\n",
    "\n",
    "tuple_list = [tuple_ for tuple_ in city_map.values()]\n",
    "\n",
    "# Create the GeoDataFrame for your points from the tuple_list\n",
    "gdf_cities = gpd.GeoDataFrame(tuple_list, geometry=gpd.points_from_xy([tuple_[2] for tuple_ in tuple_list], [tuple_[1] for tuple_ in tuple_list]), crs=cali_shp.crs)\n",
    "gdf_cities = gdf_cities.to_crs(\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not want to map more geographical features like counties.\n",
    "# I think the distance from large cities is enough\n",
    "\n",
    "# let's create a new column for the distance from large cities\n",
    "# first we'll plot large cities and their population\n",
    "\n",
    "gdf_cities[['pop_april_1980','pop_april_1990','pop_april_2000','pop_april_2010']] = gdf_cities[['pop_april_1980','pop_april_1990','pop_april_2000','pop_april_2010']].astype(float)\n",
    "gdf_cities['Large_City'] = gdf_cities['pop_april_2010'] > 250000\n",
    "large_cities = gdf_cities[gdf_cities['Large_City'] == True]\n",
    "large_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cali_shp.plot(color='grey',figsize=(10, 10), alpha=0.4, edgecolor='blue')\n",
    "large_cities.plot(ax=ax, kind='scatter', x='Longitude', y='Latitude', alpha=0.4, s=large_cities['pop_april_2010'] / 10000, label='pop_april_2010', figsize=(10,10), c='pop_april_2010', cmap=plt.get_cmap('jet'), colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# List of years to plot\n",
    "years = ['pop_april_1980','pop_april_1990','pop_april_2000','pop_april_2010']\n",
    "\n",
    "# Iterate over years and axes together\n",
    "for ax, year in zip(axs, years):\n",
    "    cali_shp.plot(color='grey', ax=ax, alpha=0.4, edgecolor='blue')\n",
    "    large_cities.plot(kind='scatter', x='Longitude', y='Latitude', alpha=0.4, \n",
    "                      s=large_cities[year] / 10000, label=year, ax=ax, \n",
    "                      c=large_cities[year], cmap=plt.get_cmap('jet'), colorbar=True)\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the distance from large cities\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "large_cities_lat_lon = large_cities[['Name','Latitude', 'Longitude']]\n",
    "large_cities_dictionaries = large_cities_lat_lon.to_dict('records', into=OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need two tuples of lat and lon for city and location\n",
    "\n",
    "for large_city in large_cities_dictionaries:\n",
    "    large_city_base_name = large_city['Name']\n",
    "    large_city_base_name = large_city_base_name.replace(' ', '_').lower()\n",
    "    large_city_enriched_name = large_city_base_name + '_km_distance'\n",
    "    gdf[large_city_enriched_name] = gdf.apply(lambda row: geodesic((row['latitude'], row['longitude']), (large_city['Latitude'], large_city['Longitude'])).kilometers, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all column names that end with '_km_distance'\n",
    "# parsed_string = s.split('_km_distance')[0].replace('_', ' ')\n",
    "# distance_cols = [(col.split('_km_distance')[0].replace('_', ' '), col) for col in gdf.columns if col.endswith('_km_distance')]\n",
    "distance_cols = [col for col in gdf.columns if col.endswith('_km_distance')]\n",
    "distance_cols\n",
    "\n",
    "# Select only these columns\n",
    "distance_data = gdf[distance_cols]\n",
    "distance_data.head()    \n",
    "\n",
    "# # Find the minimum distance for each row\n",
    "gdf['min_distance'] = distance_data.min(axis=1)\n",
    "gdf['max_distance'] = distance_data.max(axis=1)\n",
    "\n",
    "# Find the column name where column equals the min for each row\n",
    "# column name must be unique in terms of distance_km since we're going to need to regex that and drop those columns to save space\n",
    "gdf['min_distance_km_col'] = distance_data.idxmin(axis=1)\n",
    "gdf['max_distance_km_col'] = distance_data.idxmax(axis=1)\n",
    "\n",
    "gdf.drop(columns=distance_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['min_distance_city_name'] = gdf['min_distance_km_col'].apply(lambda x: x.split('_km_distance')[0].replace('_', ' '))\n",
    "gdf.drop(columns=['min_distance_km_col', 'max_distance_km_col', 'max_distance'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.drop(columns=['latitude', 'longitude', 'max_distance_city_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single subplot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "# cali_shp.plot(color='grey', ax=ax, alpha=0.4, edgecolor='blue')\n",
    "\n",
    "# Plot the geometry column of gdf on the ax\n",
    "gdf.plot(column='min_distance_city_name', ax=ax, alpha=0.4, \n",
    "         edgecolor='white', legend=True, cmap='jet')\n",
    "\n",
    "# Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cali_shp.plot(color='grey',figsize=(10, 10), alpha=0.4, edgecolor='blue')\n",
    "\n",
    "# Plot the geometry column of gdf on the ax\n",
    "gdf.plot(column='min_distance_city_name', ax=ax, alpha=0.4, \n",
    "         edgecolor='white', legend=True, cmap='jet')\n",
    "\n",
    "# Show the plot\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to get the population of the city that is closest to each row\n",
    "# We can do this by merging the gdf with the large_cities dataframe\n",
    "# We'll need to rename the columns in large_cities to match the column names in gdf\n",
    "# We'll also need to drop the geometry column from large_cities\n",
    "# We'll also need to drop the geometry column from gdf\n",
    "\n",
    "# Rename the columns in large_cities\n",
    "# large_cities = large_cities.rename(columns={'min_distance_city_name': 'city_name'})\n",
    "# large_cities['min_distance_city_name'] = large_cities['min_distance_city_name'].apply(lambda x: x.lower())\n",
    "# large_cities.reset_index(inplace=True)\n",
    "# large_cities.drop(columns=['index'], inplace=True)\n",
    "\n",
    "large_cities = large_cities.rename(columns={'Name': 'city_name_pops'})\n",
    "large_cities['city_name_pops'] = large_cities['city_name_pops'].apply(lambda x: x.lower())\n",
    "large_cities['city_name_pops'] = large_cities['city_name_pops'].apply(lambda x: x.replace(' ', '_'))\n",
    "large_cities.head(len(large_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['min_distance_city_name'] = gdf['min_distance_city_name'].apply(lambda x: x.replace(' ', '_'))\n",
    "gdf['min_distance_city_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = gdf.merge(large_cities, how='left', left_on='min_distance_city_name', right_on='city_name_pops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.drop(columns=['city_name_pops', 'Latitude', 'Longitude', 'geometry_y', 'Large_City'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.rename(columns={'geometry_x': 'geometry'}, inplace=True)\n",
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.drop(columns=['geometry'], inplace=True)\n",
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to feature engineer ocean_proximity to be a categorical variable, and then one-hot encode it\n",
    "# We'll also need to drop the ocean_proximity column\n",
    "# We'll also need to remove outliers from columns as well\n",
    "number_cols = merged_gdf.select_dtypes(include=np.number)\n",
    "# let's look at the scatter plots of the variables that are highly correlated\n",
    "sns.pairplot(number_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas\n",
    "# merged_gdf.hist(bins=30, layout=(merged_gdf.shape[1], 1), figsize=(6, 6*merged_gdf.shape[1]))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Using seaborn\n",
    "for column in merged_gdf.select_dtypes(include=np.number):\n",
    "    plt.figure()\n",
    "    sns.histplot(merged_gdf[column], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based off of these histograms, we need to normalize the following columns:\n",
    "# log for population of each city\n",
    "# pop_april_1980 pop_april_1990 pop_april_2000 pop_april_2010\n",
    "\n",
    "merged_gdf[['pop_april_1980', 'pop_april_1990', 'pop_april_2000', 'pop_april_2010']] = merged_gdf[['pop_april_1980', 'pop_april_1990', 'pop_april_2000', 'pop_april_2010']].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merged_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processor = Pipeline([(\"std_scaler\", StandardScaler())])\n",
    "\n",
    "cat_processor = Pipeline(\n",
    "    [(\"one_hot_encoder\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"num\",\n",
    "            num_processor,\n",
    "            [\n",
    "                \"housing_median_age\",\n",
    "                \"population\",\n",
    "                \"median_income\",\n",
    "                \"median_house_value\",\n",
    "                \"rooms_per_household\",\n",
    "                \"bedrooms_per_room\",\n",
    "                \"population_per_household\",\n",
    "                \"min_distance\",\n",
    "                \"pop_april_1980\",\n",
    "                \"pop_april_1990\",\n",
    "                \"pop_april_2000\",\n",
    "                \"pop_april_2010\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"cat\", cat_processor, [\"min_distance_city_name\", \"ocean_proximity\"]),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polars311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
